{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 1,
   "source": [
    "Clean up numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = open('../data/shittalkCleaned.txt', 'w')\n",
    "inFile = open('../data/shittalk.txt')\n",
    "for line in inFile:\n",
    "    outFile.write( line.split(\"|\")[1] )"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Convert into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 49: expected 1 fields, saw 2\nSkipping line 296: expected 1 fields, saw 2\nSkipping line 556: expected 1 fields, saw 2\n\n"
     ]
    }
   ],
   "source": [
    "#error_bad_lines=False => somehow some lines can't be read\n",
    "dataOnlyText = pd.read_table('../data/shittalkCleaned.txt', error_bad_lines=False)\n",
    "dataOnlyText['Rating'] = 'none'\n",
    "dataOnlyText['keywords'] = ''\n",
    "dataOnlyText['count'] = 0\n",
    "dataOnlyText.to_csv('../data/ShitTalkTable.csv')"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "dataTable = pd.read_csv('../data/ShitTalkTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "textTest = dataTable['Text'][0]"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Delete bad jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import insulterFromDatabase as ai\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "dataTable = pd.read_csv('../../data/ShitTalkTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-do the dict assignment after deletion\n",
    "keyDict = ai.putKeywordsToDict( dataTable )\n",
    "len( keyDict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c5de912e4dc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/ShitTalkTable_Filtered.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataTable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/ShitTalkTable_Filtered.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mkeyDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputKeywordsToDict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdataTable\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m# re-do the dict assignment after deletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mkeyDict\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sonneundasche/programming/of/apps/ElisaKora/main/pythonChatter/insulterFromDatabase.pyc\u001b[0m in \u001b[0;36mputKeywordsToDict\u001b[0;34m(dataFrame)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtext_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mkeywords_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetKeywords\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtext_\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeywords_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mdict_\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0m_key\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sonneundasche/programming/of/apps/ElisaKora/main/pythonChatter/insulterFromDatabase.pyc\u001b[0m in \u001b[0;36mgetKeywords\u001b[0;34m(textIn)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# -----------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetKeywords\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtextIn\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtokens_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtextIn\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mtagged_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtokens_\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sonneundasche/programming/PythonVirEnv/SkipThoughtVector/lib/python2.7/site-packages/nltk/tokenize/regexp.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# If our regexp matches tokens, use re.findall:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "filterWord = 'aim'\n",
    "# dataTable = dataTable.drop( dataTable.index[ keyDict[ filterWord ] ] )\n",
    "dataTable.to_csv('../../data/ShitTalkTable_Filtered.csv')\n",
    "# dataTable = pd.read_csv('../../data/ShitTalkTable_Filtered.csv')\n",
    "keyDict = ai.putKeywordsToDict( dataTable ) # re-do the dict assignment after deletion\n",
    "print( len(dataTable.index))\n",
    "print( len( keyDict ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterWord = 'turn'\n",
    "                      \n",
    "for rowNr in keyDict[ filterWord ] :\n",
    "    dataTable.set_value( rowNr, 'Rating', 'shit')\n",
    "    # print dataTable.iloc[ rowNr ]\n",
    "    \n",
    "print len( keyDict[ filterWord ] ) \n",
    "dataTable[ dataTable.Rating == 'shit' ].Rating.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 dollar\n3 shot\n2 sniper\n2 fan\n2 school\n4 level\n2 die\n7 button\n2 list\n2 crosshair\n5 guy\n2 round\n2 existance\n3 jump\n3 video\n2 perspective\n2 cactus\n2 wait\n4 coffin\n3 amount\n3 gene\n4 family\n2 excuse\n3 everybody\n2 use\n2 insult\n2 type\n2 today\n3 phone\n2 shoot\n2 soup\n2 example\n2 control\n5 salty\n15 something\n2 sense\n2 turn\n4 map\n3 sandwich\n5 lag\n3 man\n2 Everyone\n2 talk\n2 refund\n3 sperm\n6 brain\n3 existence\n2 bunch\n5 monitor\n2 policy\n6 dad\n4 someone\n4 day\n2 didn\n3 mode\n2 everyone\n2 idea\n11 thing\n3 internet\n3 cause\n3 highway\n8 ass\n3 difficulty\n11 reason\n3 hand\n3 place\n2 router\n2 number\n2 system\n2 option\n2 tree\n2 mind\n2 argument\n5 ego\n2 asshole\n2 prick\n9 mouth\n9 don\n5 face\n2 Earth\n7 salt\n2 selection\n2 hunger\n2 crayon\n2 ground\n3 dick\n3 condom\n5 awareness\n2 enemy\n5 evolution\n6 computer\n6 screen\n3 coma\n9 joke\n3 job\n3 s\n2 point\n2 wall\n10 life\n4 fire\n2 look\n3 value\n3 cock\n2 layer\n2 im\n9 mouse\n13 shit\n2 retard\n7 i\n7 person\n7 mother\n3 sentence\n3 skill\n2 coordination\n2 generator\n2 character\n4 mom\n5 world\n3 trash\n24 t\n2 right\n7 fucking\n6 tutorial\n2 participation\n3 power\n2 eye\n2 sister\n2 garbage\n6 support\n2 grave\n5 head\n3 count\n3 dog\n9 chromosome\n3 vocabulary\n2 handicap\n4 chance\n3 pool\n2 potato\n23 time\n"
     ]
    }
   ],
   "source": [
    "keyDictFiltered = ai.putKeywordsToDict( dataTable[ dataTable.Rating != 'shit' ] )\n",
    "# pair = keyDictFiltered[ 'amount' ]\n",
    "# pair\n",
    "for k in keyDictFiltered :\n",
    "   length = len( keyDictFiltered[ k ] )\n",
    "   if length > 1 :\n",
    "       print str(length) + \" \" + str(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTable.to_csv('../../data/ShitTalkTable_Filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why turn on the computer if you're just gonna make it depressed\nYou know this isn't turn based combat right? When they shoot at you feel free to shoot back.\n"
     ]
    }
   ],
   "source": [
    "filterWord = 'turn'\n",
    "for rowNr in keyDictFiltered[ filterWord ] :\n",
    "    print dataTable.loc[ rowNr ].Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}