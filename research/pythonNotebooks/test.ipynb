{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from subprocess import call\n",
    "import os\n",
    "os.environ['PATH'] += os.pathsep + '/usr/local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"A man and a woman standing in the kitchen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'man', 'and', 'a', 'woman', 'standing', 'in', 'the', 'kitchen']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('man', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('a', 'DT'),\n",
       " ('woman', 'NN'),\n",
       " ('standing', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('kitchen', 'NN')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "insults = [\"ugly\", \"fat\", \"smelly\",\"greasy\",\"dirty\",\"stupid as fuck\",\"lame\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = []\n",
    "for currentpair in tagged:\n",
    "    if (currentpair[1]=='NN'): transformed.append(random.choice(insults))\n",
    "    transformed.append(currentpair[0])\n",
    "call([\"say\",str(transformed)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'dirty', 'man', 'and', 'a', 'fat', 'woman', 'standing', 'in', 'the', 'greasy', 'kitchen']\n"
     ]
    }
   ],
   "source": [
    "print transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Used when tokenizing words\n",
    "sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "      ([A-Z])(\\.[A-Z])+\\.?  # abbreviations, e.g. U.S.A.\n",
    "    | \\w+(-\\w+)*            # words with optional internal hyphens\n",
    "    | \\$?\\d+(\\.\\d+)?%?      # currency and percentages, e.g. $12.40, 82%\n",
    "    | \\.\\.\\.                # ellipsis\n",
    "    | [][.,;\"'?():-_`]      # these are separate tokens\n",
    "'''\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "#Taken from Su Nam Kim Paper...\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\"\n",
    "chunker = nltk.RegexpParser(grammar)\n",
    "\n",
    "toks = nltk.regexp_tokenize(sentence, sentence_re)\n",
    "postoks = nltk.tag.pos_tag(tokens)\n",
    "tree = chunker.parse(postoks)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = stemmer.stem(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        yield term\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "terms = get_terms(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man\n",
      "woman\n",
      "kitchen\n"
     ]
    }
   ],
   "source": [
    "for term in terms:\n",
    "    for word in term:\n",
    "        print word,\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    " rd_parser = nltk.RecursiveDescentParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sent = 'mary saw a dog'.split()\n",
    "for tr in rd_parser.parse(sent):\n",
    "    print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I]\n",
      "('lemma:', I, u'nsubj')\n",
      "('lemma:', shot, u'ROOT')\n",
      "('lemma:', an, u'det')\n",
      "('lemma:', elephant, u'dobj')\n"
     ]
    }
   ],
   "source": [
    "from spacy.en import English\n",
    "parser = English()\n",
    "usnt = unicode('I shot an elephant')\n",
    "#usnt = unicode(sentence)\n",
    "parsedData=parser(usnt)\n",
    "\n",
    "sub_toks = [tok for tok in parsedData if (tok.dep_ == \"nsubj\") ]\n",
    "print sub_toks\n",
    "for i, token1 in enumerate(parsedData):\n",
    "    print(\"lemma:\", token1, token1.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40d0cefba8b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The boy with the spotted dog quickly ran after the firetruck.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparsedEx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsedEx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlefts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "example = unicode(\"The boy with the spotted dog quickly ran after the firetruck.\")\n",
    "parsedEx = parser(unicode(sentence))\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------\n",
      "Top 3 closest results for king - man + woman:\n",
      "queen\n",
      "prince\n",
      "kings\n"
     ]
    }
   ],
   "source": [
    "# Let's see if it can figure out this analogy\n",
    "# Man is to King as Woman is to ??\n",
    "king = parser.vocab[unicode('king')]\n",
    "man = parser.vocab[unicode('man')]\n",
    "woman = parser.vocab[unicode('woman')]\n",
    "\n",
    "result = king.vector - man.vector + woman.vector\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"king\" and w.lower_ != \"man\" and w.lower_ != \"woman\"})\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.vector, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar words to NASA:\n",
      "ugly\n",
      "hideous\n",
      "awful\n",
      "fugly\n",
      "horrible\n",
      "disgusting\n",
      "stupid\n",
      "unattractive\n",
      "nasty\n",
      "horrid\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# you can access known words from the parser's vocabulary\n",
    "nasa = parser.vocab[unicode('ugly')]\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"nasa\"})\n",
    "\n",
    "# sort by similarity to NASA\n",
    "allWords.sort(key=lambda w: cosine(w.vector, nasa.vector))\n",
    "allWords.reverse()\n",
    "print(\"Top 10 most similar words to NASA:\")\n",
    "for word in allWords[:10]:   \n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------\n",
      "Top 3 closest results for king - man + woman:\n",
      "lady\n",
      "girl\n",
      "boy\n",
      "men\n",
      "guy\n",
      "young\n",
      "mother\n",
      "she\n",
      "women\n",
      "gentleman\n",
      "wife\n",
      "he\n",
      "teenager\n",
      "husband\n",
      "father\n",
      "stranger\n",
      "person\n",
      "someone\n",
      "female\n",
      "who\n",
      "victim\n",
      "male\n",
      "sex\n",
      "lover\n",
      "him\n",
      "himself\n",
      "herself\n",
      "sexy\n",
      "girlfriend\n",
      "her\n",
      "teenage\n",
      "knows\n",
      "dead\n",
      "one\n",
      "friend\n",
      "when\n",
      "gay\n",
      "wants\n",
      "mom\n",
      "thing\n",
      "girls\n",
      "policeman\n",
      "life\n",
      "dude\n",
      "cop\n",
      "mans\n",
      "boyfriend\n",
      "soldier\n",
      "never\n",
      "pregnant\n",
      "ladies\n",
      "dad\n",
      "drunk\n",
      "knew\n",
      "naked\n",
      "being\n",
      "ever\n",
      "nothing\n",
      "somebody\n",
      "suspect\n",
      "kind\n",
      "his\n",
      "fuck\n",
      "say\n",
      "teen\n",
      "lad\n",
      "face\n",
      "whom\n",
      "brother\n",
      "prostitute\n",
      "lives\n",
      "thought\n",
      "fucking\n",
      "mind\n",
      "told\n",
      "that\n",
      "way\n",
      "alone\n",
      "sexual\n",
      "apparently\n",
      "raped\n",
      "looking\n",
      "tells\n",
      "housewife\n",
      "horny\n",
      "daughter\n",
      "killed\n",
      "lonely\n",
      "tell\n",
      "finds\n",
      "blonde\n",
      "boys\n",
      "was\n",
      "telling\n",
      "innocent\n",
      "wonder\n",
      "once\n",
      "death\n",
      "couple\n",
      "story\n"
     ]
    }
   ],
   "source": [
    "# Let's see if it can figure out this analogy\n",
    "# Man is to King as Woman is to ??\n",
    "king = parser.vocab[unicode('ugly')]\n",
    "man = parser.vocab[unicode('man')]\n",
    "woman = parser.vocab[unicode('woman')]\n",
    "\n",
    "#result = king.vector*man.vector\n",
    "result = woman.vector+man.vector\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"king\" and w.lower_ != \"man\" and w.lower_ != \"woman\"})\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.vector, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:100]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local Python",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
