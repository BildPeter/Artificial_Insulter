{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from subprocess import call\n",
    "import os\n",
    "os.environ['PATH'] += os.pathsep + '/usr/local/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"A man and a woman standing in the kitchen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'man', 'and', 'a', 'woman', 'standing', 'in', 'the', 'kitchen']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('man', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('a', 'DT'),\n",
       " ('woman', 'NN'),\n",
       " ('standing', 'VBG'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('kitchen', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "insults = [\"ugly\", \"fat\", \"smelly\",\"greasy\",\"dirty\",\"stupid as fuck\",\"lame\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = []\n",
    "for currentpair in tagged:\n",
    "    if (currentpair[1]=='NN'): transformed.append(random.choice(insults))\n",
    "    transformed.append(currentpair[0])\n",
    "call([\"say\",str(transformed)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'stupid as fuck', 'man', 'and', 'a', 'ugly', 'woman', 'standing', 'in', 'the', 'smelly', 'kitchen']\n"
     ]
    }
   ],
   "source": [
    "print transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Used when tokenizing words\n",
    "sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "      ([A-Z])(\\.[A-Z])+\\.?  # abbreviations, e.g. U.S.A.\n",
    "    | \\w+(-\\w+)*            # words with optional internal hyphens\n",
    "    | \\$?\\d+(\\.\\d+)?%?      # currency and percentages, e.g. $12.40, 82%\n",
    "    | \\.\\.\\.                # ellipsis\n",
    "    | [][.,;\"'?():-_`]      # these are separate tokens\n",
    "'''\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "#Taken from Su Nam Kim Paper...\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\"\n",
    "chunker = nltk.RegexpParser(grammar)\n",
    "\n",
    "toks = nltk.regexp_tokenize(sentence, sentence_re)\n",
    "postoks = nltk.tag.pos_tag(tokens)\n",
    "tree = chunker.parse(postoks)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = stemmer.stem(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        yield term\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "terms = get_terms(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for term in terms:\n",
    "    for word in term:\n",
    "        print word,\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objects: ['kitchen']\n",
      "persons: ['man', 'woman']\n"
     ]
    }
   ],
   "source": [
    "# analyse every term in a sentence and extract persons and objects:\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wn_entity = wn.synset('entity.n.01')\n",
    "wn_physical_entity = wn.synset('physical_entity.n.01')\n",
    "wn_causal_agent = wn.synset('causal_agent.n.01')\n",
    "wn_person = wn.synset('person.n.01')\n",
    "wn_object = wn.synset('object.n.01')\n",
    "# person hierarchy: entity->ph_entity->cas_agent->person\n",
    "# object hierarchy: entity->ph_entity->object\n",
    "persons = []\n",
    "objects = []\n",
    "maxdepth = 4\n",
    "for term in get_terms(tree):\n",
    "    for word in term:\n",
    "        synset = wn.synsets(word)[0].hypernym_paths()[0]\n",
    "        #print word, \":\"\n",
    "        #print synset,\n",
    "        if (wn_object in synset[:maxdepth]): objects.append(word)\n",
    "        if (wn_person in synset[:maxdepth]): persons.append(word)\n",
    "\n",
    "print \"objects:\",objects\n",
    "print \"persons:\",persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "res_similarity() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-9f8196dbe953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'man.n.01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: res_similarity() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "print wn.synsets('woman')[0].res_similarity(wn.synset('man.n.01'))\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "rd_parser = nltk.RecursiveDescentParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'check_coverage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-1a3e8775e38d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/nltk/parse/recursivedescent.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Start a recursive descent parse, with an initial tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'check_coverage'"
     ]
    }
   ],
   "source": [
    "for tr in rd_parser.parse(entities):\n",
    "    print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "(A, u'det')\n",
      "(man, u'ROOT')\n",
      "(and, u'cc')\n",
      "(a, u'det')\n",
      "(woman, u'conj')\n",
      "(standing, u'acl')\n",
      "(in, u'prep')\n",
      "(the, u'det')\n",
      "(kitchen, u'pobj')\n"
     ]
    }
   ],
   "source": [
    "from spacy.en import English\n",
    "parser = English()\n",
    "#usnt = unicode('I shot an elephant')\n",
    "usnt = unicode(sentence)\n",
    "parsedData=parser(usnt)\n",
    "#nsubjpass\n",
    "sub_toks = [tok for tok in parsedData if (tok.dep_ == \"nsubj\") ]\n",
    "print sub_toks\n",
    "for tok in parsedData:   \n",
    "        print(tok, tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40d0cefba8b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The boy with the spotted dog quickly ran after the firetruck.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparsedEx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsedEx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlefts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morth_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "example = unicode(\"The boy with the spotted dog quickly ran after the firetruck.\")\n",
    "parsedEx = parser(unicode(sentence))\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------\n",
      "Top 3 closest results for king - man + woman:\n",
      "queen\n",
      "prince\n",
      "kings\n"
     ]
    }
   ],
   "source": [
    "# Let's see if it can figure out this analogy\n",
    "# Man is to King as Woman is to ??\n",
    "king = parser.vocab[unicode('king')]\n",
    "man = parser.vocab[unicode('man')]\n",
    "woman = parser.vocab[unicode('woman')]\n",
    "\n",
    "result = king.vector - man.vector + woman.vector\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"king\" and w.lower_ != \"man\" and w.lower_ != \"woman\"})\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.vector, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:3]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most similar words to NASA:\n",
      "ugly\n",
      "hideous\n",
      "awful\n",
      "fugly\n",
      "horrible\n",
      "disgusting\n",
      "stupid\n",
      "unattractive\n",
      "nasty\n",
      "horrid\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# you can access known words from the parser's vocabulary\n",
    "nasa = parser.vocab[unicode('ugly')]\n",
    "\n",
    "# cosine similarity\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"nasa\"})\n",
    "\n",
    "# sort by similarity to NASA\n",
    "allWords.sort(key=lambda w: cosine(w.vector, nasa.vector))\n",
    "allWords.reverse()\n",
    "print(\"Top 10 most similar words to NASA:\")\n",
    "for word in allWords[:10]:   \n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------\n",
      "Top 3 closest results for king - man + woman:\n",
      "lady\n",
      "girl\n",
      "boy\n",
      "men\n",
      "guy\n",
      "young\n",
      "mother\n",
      "she\n",
      "women\n",
      "gentleman\n",
      "wife\n",
      "he\n",
      "teenager\n",
      "husband\n",
      "father\n",
      "stranger\n",
      "person\n",
      "someone\n",
      "female\n",
      "who\n",
      "victim\n",
      "male\n",
      "sex\n",
      "lover\n",
      "him\n",
      "himself\n",
      "herself\n",
      "sexy\n",
      "girlfriend\n",
      "her\n",
      "teenage\n",
      "knows\n",
      "dead\n",
      "one\n",
      "friend\n",
      "when\n",
      "gay\n",
      "wants\n",
      "mom\n",
      "thing\n",
      "girls\n",
      "policeman\n",
      "life\n",
      "dude\n",
      "cop\n",
      "mans\n",
      "boyfriend\n",
      "soldier\n",
      "never\n",
      "pregnant\n",
      "ladies\n",
      "dad\n",
      "drunk\n",
      "knew\n",
      "naked\n",
      "being\n",
      "ever\n",
      "nothing\n",
      "somebody\n",
      "suspect\n",
      "kind\n",
      "his\n",
      "fuck\n",
      "say\n",
      "teen\n",
      "lad\n",
      "face\n",
      "whom\n",
      "brother\n",
      "prostitute\n",
      "lives\n",
      "thought\n",
      "fucking\n",
      "mind\n",
      "told\n",
      "that\n",
      "way\n",
      "alone\n",
      "sexual\n",
      "apparently\n",
      "raped\n",
      "looking\n",
      "tells\n",
      "housewife\n",
      "horny\n",
      "daughter\n",
      "killed\n",
      "lonely\n",
      "tell\n",
      "finds\n",
      "blonde\n",
      "boys\n",
      "was\n",
      "telling\n",
      "innocent\n",
      "wonder\n",
      "once\n",
      "death\n",
      "couple\n",
      "story\n"
     ]
    }
   ],
   "source": [
    "# Let's see if it can figure out this analogy\n",
    "# Man is to King as Woman is to ??\n",
    "king = parser.vocab[unicode('ugly')]\n",
    "man = parser.vocab[unicode('man')]\n",
    "woman = parser.vocab[unicode('woman')]\n",
    "\n",
    "#result = king.vector*man.vector\n",
    "result = woman.vector+man.vector\n",
    "\n",
    "# gather all known words, take only the lowercased versions\n",
    "allWords = list({w for w in parser.vocab if w.has_vector and w.orth_.islower() and w.lower_ != \"king\" and w.lower_ != \"man\" and w.lower_ != \"woman\"})\n",
    "# sort by similarity to the result\n",
    "allWords.sort(key=lambda w: cosine(w.vector, result))\n",
    "allWords.reverse()\n",
    "print(\"\\n----------------------------\\nTop 3 closest results for king - man + woman:\")\n",
    "for word in allWords[:100]:   \n",
    "    print(word.orth_)\n",
    "    \n",
    "# it got it! Queen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shot]\n"
     ]
    }
   ],
   "source": [
    "usnt = unicode('I shot an elephant')\n",
    "#usnt = unicode(sentence)\n",
    "\n",
    "from spacy.symbols import nsubj, VERB\n",
    "# Finding a verb with a subject from below — good\n",
    "\n",
    "verbs = []\n",
    "for possible_verb in parser(usnt):\n",
    "    if possible_verb.pos == VERB:\n",
    "        for possible_subject in possible_verb.children:\n",
    "            if possible_subject.dep == nsubj:\n",
    "                verbs.append(possible_verb)\n",
    "                break\n",
    "                \n",
    "print verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'A', 469, u'a', 426, u'DT', 87, u'DET')\n",
      "(u'man', 698, u'man', 440, u'NN', 89, u'NOUN')\n",
      "(u'and', 470, u'and', 424, u'CC', 86, u'CONJ')\n",
      "(u'a', 469, u'a', 426, u'DT', 87, u'DET')\n",
      "(u'woman', 1074, u'woman', 440, u'NN', 89, u'NOUN')\n",
      "(u'standing', 1299, u'stand', 456, u'VBG', 97, u'VERB')\n",
      "(u'in', 477, u'in', 432, u'IN', 83, u'ADP')\n",
      "(u'the', 466, u'the', 426, u'DT', 87, u'DET')\n",
      "(u'kitchen', 3633, u'kitchen', 440, u'NN', 89, u'NOUN')\n"
     ]
    }
   ],
   "source": [
    "usnt = unicode(sentence)\n",
    "for word in parser(usnt):\n",
    "    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'A man', u'man', u'ROOT', u'man')\n",
      "(u'a woman', u'woman', u'conj', u'man')\n",
      "(u'the kitchen', u'kitchen', u'pobj', u'in')\n"
     ]
    }
   ],
   "source": [
    "# this one is cool:\n",
    "usnt = unicode(sentence)\n",
    "for np in parser(usnt).noun_chunks:\n",
    "    print(np.text, np.root.text, np.root.dep_, np.root.head.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN man\n",
      "  DT A\n",
      "  CC and\n",
      "  NN woman\n",
      "    DT a\n",
      "    VBG standing\n",
      "      IN in\n",
      "        NN kitchen\n",
      "          DT the\n"
     ]
    }
   ],
   "source": [
    "def get_root(doc):\n",
    "    tok = doc[0]\n",
    "    while tok != tok.head:\n",
    "        tok = tok.head\n",
    "\n",
    "    return tok\n",
    "\n",
    "def print_node(nlp, node, indent):\n",
    "    print((' ' * indent) + nlp.vocab.strings[node.tag] + ' ' + str(node))\n",
    "    for tok in node.children:\n",
    "        if tok != node:\n",
    "            print_node(nlp, tok, indent + 2)\n",
    "\n",
    "def print_doc(nlp, doc):\n",
    "    root = get_root(doc)\n",
    "    print_node(nlp, root, indent=0)\n",
    "    \n",
    "print_doc(parser,parser(usnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('artifact.n.01'),\n",
       " Synset('structure.n.01'),\n",
       " Synset('area.n.05'),\n",
       " Synset('room.n.01'),\n",
       " Synset('kitchen.n.01')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "# take first item for word 'kitchen'\n",
    "kitchen = wn.synsets('kitchen')[0]\n",
    "paths = kitchen.hypernym_paths()\n",
    "paths[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got line: sadf\n",
      "got line: ggasd\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-0c856dadf846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipein\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipein\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"got line:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, time,sys\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "pipe_name = home+'/text_transform'\n",
    "pipein = open(pipe_name, 'r')\n",
    "while True:\n",
    "        line = pipein.readline()[:-1]\n",
    "        print \"got line:\",line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nesa/asdf'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local Python",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
